---
title: "The UnHealth Dashboard: Part 2"
date: 2024-05-24 12:00:00 +0000
categories: [Plotly Dash, Public Health, Dashboard, Python]
tags: [Dash App, Portfolio, Project, Tutorial, UnHealth Dashboard, directory structure, GeoPandas, CDC PLACES]
---

# Getting and Processing the Data

I wanted to build a Dashboard related to public health for a portfolio project. 

Eventually, this became [The UnHealth Dashboard](https://bit.ly/UnHealthDashboard). 

It's deployed on Heroku so if the app is asleep it may take about 20 seconds to wake up.

This post is the second in a series, explaining the process of getting the data and processing it for the dashboard. 

The GitHub repo for this project is here: [The Unhealth Dashboard Repo](https://github.com/SloughJE/UnHealth_Dashboard/)

## The Data: Downloading and Initial Processing

Our main dataset is CDC PLACES. 

I mentioned in the previous post how I have the `run_pipelines.py` file to manage all the pipelines. Let's use that to help explain the data loading and processing steps. 

Our first step is running `python run_pipelines.py --get_CDC_PLACES_data`. This will run the function `get_cdc_places_data` which just downloads the 2022 and 2023 PLACES csv from the direct link and saves it to disk as a pickle file. The `--get_CDC_PLACES_data` is a command line argument for the `run_pipelines.py` file which indicates that it should run the `get_cdc_places_data` function. This is set up by:

```python
    parser.add_argument(
        "--get_CDC_PLACES_data",
        help="load and process raw data from CDC",
        action="store_true"
    )
```
and:
```python
        if args.get_CDC_PLACES_data:
            get_cdc_places_data(
                link_2022_csv = "https://data.cdc.gov/api/views/duw2-7jbt/rows.csv?accessType=DOWNLOAD",
                link_2023_csv = "https://data.cdc.gov/api/views/swc5-untb/rows.csv?accessType=DOWNLOAD"
            )
```
We can follow the same pattern to add more arguments and functions.

Here's an extract of the raw data from the CSV.

|   Year | StateAbbr   | StateDesc   | LocationName   | DataSource   | Category        | Measure                                           | Data_Value_Unit   | Data_Value_Type         |   Data_Value | Data_Value_Footnote_Symbol   | Data_Value_Footnote   |   Low_Confidence_Limit |   High_Confidence_Limit |   TotalPopulation |   LocationID | CategoryID   | MeasureId   | DataValueTypeID   | Short_Question_Text   | Geolocation                    |
|-------:|:------------|:------------|:---------------|:-------------|:----------------|:--------------------------------------------------|:------------------|:------------------------|-------------:|:-----------------------------|:----------------------|-----------------------:|------------------------:|------------------:|-------------:|:-------------|:------------|:------------------|:----------------------|:-------------------------------|
|   2021 | GA          | Georgia     | Ware           | BRFSS        | Health Outcomes | Stroke among adults aged >=18 years               | %                 | Crude prevalence        |          4.6 |                              |                       |                    4   |                     5.1 |             36033 |        13299 | HLTHOUT      | STROKE      | CrdPrv            | Stroke                | POINT (-82.4215072 31.050881)  |
|   2021 | IN          | Indiana     | Ohio           | BRFSS        | Health Outcomes | Stroke among adults aged >=18 years               | %                 | Crude prevalence        |          3.8 |                              |                       |                    3.3 |                     4.4 |              5978 |        18115 | HLTHOUT      | STROKE      | CrdPrv            | Stroke                | POINT (-84.9642994 38.940527)  |
|   2021 | IN          | Indiana     | Owen           | BRFSS        | Health Outcomes | Stroke among adults aged >=18 years               | %                 | Crude prevalence        |          4   |                              |                       |                    3.5 |                     4.6 |             21446 |        18119 | HLTHOUT      | STROKE      | CrdPrv            | Stroke                | POINT (-86.8388453 39.3173394) |
|   2021 | AR          | Arkansas    | Benton         | BRFSS        | Disability      | Cognitive disability among adults ages >=18 years | %                 | Age-adjusted prevalence |         13.7 |                              |                       |                   11.4 |                    16.1 |            293692 |         5007 | DISABLT      | COGNITION   | AgeAdjPrv         | Cognitive Disability  | POINT (-94.2562984 36.3378189) |



Next we run `python run_pipelines.py --initial_processing_CDC_PLACES_data` to process that data. This performs some processing on the 2 CSV files that we downloaded. 

Most of the code I'm referring to is accessible [here](https://github.com/SloughJE/UnHealth_Dashboard/blob/production/src/data/load_data.py).

Some quick notes on that processing:

### Florida

Florida had no data on any of the disability metrics. If you look at the table above, you can see in the `Category` column, a value of "Disability". Florida didn't have any of that data but all other locations did. I don't know why. 

In order to have data for those metrics, which was necessary in creating my `UnHealth Score` (more on that later), I set Florida's disability metrics as the USA average...

### Geolocation Data

I added Geolocation data using the [GeoJSON](https://www2.census.gov/geo/tiger/GENZ2021/shp/) files of US counties from the US Census. For the US Census GeoJSON, I use GeoPandas to merge it with the PLACES data. The PLACES data has coordinates for each row but the GeoJSON has borders and other location information for each county.

The GeoJSON file has a CRS (Coordinate Reference System), of `epsg=4269` which defines how spatial data is projected onto a flat surface. We need to also set the PLACES data to the same CRS by `df_health.set_crs(epsg=4269, inplace=True)`.

---
But what are these?

The EPSG (originally the European Petroleum Survey Group) is a database of standard identifiers for CRS definitions. EPSG:4269 refers to the North American Datum 1983 (NAD83), which is widely used in North America. By setting the CRS of the PLACES data to EPSG:4269 using `df_health.set_crs(epsg=4269, inplace=True)`, we ensure that the spatial data points in the PLACES uses the same method of projection as the Census GeoJSON files so that the data aligns accurately.

In short, it's just way way to identify standards used for spatial data positioning on a map. 

---

Then I use a spatial join: `gpd.sjoin(df_health, us_counties, how='left', predicate='intersects')`, which means that if a coordinate from the PLACES data (Geolocation column) falls within a county's border, we'll associate that row from the PLACES data with the corresponding county so we align the health data with the geographic boundaries of US counties.

That merge adds the GEOID column to the PLACES dataframe. GEOID is a unique identifier assigned by the Census Bureau to each geographic entity, such as counties, in the GeoJSON shapefiles. Later on, we'll use this to help plot the data on a map using Plotly.

### Adjusting for Age

The data includes the **crude prevalence** and **age-adjusted prevalence** of all health measures. 

I chose to only use the age-adjusted prevalence for 2 reasons: 
1. For the dashboard, it simplifies the visualizations, comparisons, and interpretations by providing a single, clear measure of the variables of interest.
2. It should "[eliminate differences in observed rates that result from age differences in population composition](https://www.cdc.gov/nchs/hus/sources-definitions/age-adjustment.htm#:~:text=Age%2Dadjusted%20rates%20are%20computed,age%20differences%20in%20population%20composition)".

Let me explain reason 2 with an example:

*County A has a 40% rate of stroke.*

*County B has a 20% rate of stroke.*

Can we directly compare these two rates of stroke, i.e. 40% vs 20%? 

Sure, County A has twice that rate of stroke. 

But:

*80% of the population in County A are over the age of 65.*

*20% of the population in County B are over the age of 65.*

The problem? The incidence of strokes is highly correlated with age.

So if we just say County A has twice the rate of stroke as County B in one sense it's true, but in another, it's not. 

If we want to compare the rates of stroke between the two counties fairly, we should account for age. 

But why? 

An important reason for comparing counties is to identify effective public health programs and initiatives that contribute to improved health outcomes.

We can't just get rid of all the old people in a county in order to reduce the rate of stroke, right?

But we could, for example, implement a county-wide public health campaign to promote lifestyle changes and increase awareness of risk factors and early detection of strokes.

If one county does that and we see lower stroke rates, and another county doesn't and we see higher stroke rates, that could indicate the campaign is successfully and is worth a try.

But only if the stroke rates are adjusted for age.

For more information on the crude vs age-adjusted prevalence check out the [CDC Age adjustment](https://www.cdc.gov/nchs/hus/sources-definitions/age-adjustment.htm).

#### Adjusting for Everything

While age adjustment is important, we don't adjust for every possible factor. There are other unchangeable factors, such as genetics and baseline health conditions, that we also do not account for but may still be important. 

In general, adjusting for too many variables can make the analysis overly complex and less interpretable. 

What about adjusting for modifiable factors like access to healthcare?

Adjusting for modifiable factors like lifestyle and access to healthcare is different because these are modifiable and can be targeted in public health campaigns. 

In other words, we shouldn't remove the impact of public health campaigns, as that would prevent us from identifying their effectiveness.

#### Adjusting for Race

We know that certain diseases are more prevalent in certain races. 

So why don't we adjust for race? 

Because doing so could mask the underlying socioeconomic and health disparities. 

But how? 

Race correlates with other factors like education, economic status and access to healthcare.

Let's explain with an example:

*County A has a 37% rate of stroke.*

*County B has a 25% rate of stroke.*

Can we directly compare these two rates of stroke, i.e. 37% vs 25%? 

Sure, County A has a higher rate of stroke than County B. 

But:

*70% of the population in county A are of race GELF.*

*40% of the population in County B are of race Quagaar.*

The problem? Certain diseases, like stroke, are more prevalent in certain races.

So we adjust for race, and now:

*County A has adjusted rate of 32% for stroke.*

*County B has an adjusted rate of 31% for stroke.*

About the same, right?

But GELFs have a much poorer access to healthcare and lower socioeconomic status than Quagaars. 

So in this example, if we adjust for race, we miss these critical issues and public health campaigns may not target the root causes effectively.

In other words, by not adjusting for race, these disparities become visible, allowing public health campaigns to target the specific needs of the affected racial groups in the counties where they are most at risk. Adjusting for race would obscure these critical differences, potentially leading to less effective interventions.

---

To summarize:

**If we don't adjust the data by race and we see a difference in stroke rates, we can ask "why is there a difference?", and investigate further.**

**If we do adjust the data by race, and we do not see a difference in stroke rates, we won't ask "why" and we won't investigate further.**

However, if all other factors were equal and race was the only difference, then adjusting for race could be appropriate.


---

That was a bit of a rabbit hole.

Adjusting for things can get complicated...

---

### Interim File

The last thing we do to the data is extract only the columns needed for further analysis, invert some metrics (more on that in next post), and write out the interim file.

The interim file looks like:

|   Year | StateAbbr   | StateDesc   | LocationName          | Category        | Original_Measure                                                                       |   Data_Value | Data_Value_Unit   |   GEOID | Geolocation                     | Measure                                                               |
|-------:|:------------|:------------|:----------------------|:----------------|:---------------------------------------------------------------------------------------|-------------:|:------------------|--------:|:--------------------------------|:----------------------------------------------------------------------|
|   2020 | AL          | Alabama     | Wilcox                | Prevention      | Visits to doctor for routine checkup within the past year among adults aged >=18 years |         20.6 | %                 |   01131 | POINT (-87.3049349 31.9900824)  | No doctor visit for checkup in past year among adults aged >=18 years |
|   2020 | AK          | Alaska      | Haines                | Health Outcomes | Stroke among adults aged >=18 years                                                    |          2.9 | %                 |   02100 | POINT (-135.5757906 59.0984037) | Stroke among adults aged >=18 years                                   |
|   2020 | AK          | Alaska      | Prince of Wales-Hyder | Health Outcomes | Chronic obstructive pulmonary disease among adults aged >=18 years                     |          7.4 | %                 |   02198 | POINT (-133.1623885 55.6827733) | Chronic obstructive pulmonary disease among adults aged >=18 years    |


---

That was the data loading and initial processing of the main dataset. 

I do think the section on adjusting the rates is interesting and worth thinking about more. 

I only briefly explained it and did not even get into any details about exactly how the data is adjusted. 

JS