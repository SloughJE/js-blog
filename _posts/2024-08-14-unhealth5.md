---
title: "The UnHealth Dashboard: Part 5 - Fine Tuning the AI Patient Analysis"
date: 2024-08-14 12:00:00 +0000
math: true
categories: [Plotly Dash, Public Health, Dashboard, Python, AI, LLM]
tags: [Dash App, Portfolio, UnHealth Dashboard, UnHealth Score, CDC PLACES, AI, Fine-tuning, Llama, OpenAI]
---

# Fine Tuning the AI Patient Analysis

I wanted to build a Dashboard related to public health for a portfolio project. 

Eventually, this became [The UnHealth Dashboard](https://bit.ly/UnHealthDashboard). 

It's deployed on Heroku so if the app is asleep it may take about 20 seconds to wake up.

This post is the fifth in a series, explaining the process of attempting to fine-tune the AI Patient Analysis. 

The GitHub repo for this project is here: [The UnHealth Dashboard Repo](https://github.com/SloughJE/UnHealth_Dashboard/)

## What is Fine-Tuning a Large Language Model

Very simply, fine-tuning an LLM means making it work better with a specific set of related inputs and outputs. 

We have a set of original input text and output text like:

```
Input: Is OpenAI open?

Output: OpenAI is not a physical location or facility that you can visit like a store or office, so it isn't "open" in that sense. However, OpenAI's services and platforms, such as the API for GPT models, are accessible online to users who have registered and possibly been approved, depending on the specific service and current availability.*
```
\*actual output from GPT4 model.

But we want the model to respond in a different way, based on the same input.

```
Input: Is OpenAI open?

Output: No. In fact, it is lobbying the government arguably to become even more closed in an effort to entrench its position by creating a regulatory environment that is difficult for new or smaller players to navigate, thereby limiting competition.
```

Then when a user inputs something similar to the above, the LLM should respond more similarly to the fine-tuned output.

There are different ways to do this but we'll focus on Supervised Fine-Tuning (SFT). 

But, you may wonder, what's happening to the model? How does this change the model? What's being updated? 

The model weights. 

I don't want go into the technical details here. This post is more about the practicalities of fine-tuning the model.

### Why Fine-tune an LLM?

- To make it work better on a specific task

We want the LLM, which is trained on tons of text on tons of subjects, to perform better on our more specific task. This may mean that it will perform slightly worse on unrelated tasks.

Performance-wise, there would be no reason to fine-tune a model if it already performed as well as needed for our task. 

That doesn't mean that practically there is no reason to fine-tune a model. There are many examples where it might be best to fine-tune a model even it the base model works well enough. For example, perhaps we do not want the model used for anything but our specific task. 

One of the reasons fine-tuning became somewhat widespread is that the open source models have performed worse than closed source models from, for example, **Open**AI.

Fine-tuning a smaller or less advanced model for one task, may result in it performing as good or close to as good as a larger and/or more advanced base model. 

If we do not want to send data to an API for a closed source model, we could host our own open source model.

This would be especially relevant for proprietary company data or medical data, like electronic health records, which is exactly what my AI Patient Summary would need.

So, to avoid sending sensitive data to a company like OpenAI, we can use a smaller open source model, and augment its performance by fine-tuning it.

Let's try it.

## Fine-Tuning the LLama Model using Amazon Sagemaker

I'll be following the tutorial from [Phil Schmid](https://www.philschmid.de/sagemaker-train-evalaute-llms-2024) as it looked pretty straightforward, and I want to do this on Sagemaker.

I won't go step by step through this tutorial, but I will note the important parts and any modifications I used for my case. 

### The Fine-Tuning Dataset

We need the input and output data. This is 'supervised' fine-tuning so, similar to traditional supervised machine learning, the output is like the 'label' or the 'y' we want to predict. The input is then, the 'input features' or 'x'.

Remember that my [AI Patient Analysis](https://www.jsdatascience.com/blog/posts/unhealth4/) creates a summary of a patient's medical status based on their medical history, current health status, informed by the health metrics of the country they live in. It would be used to aid a doctor when seeing the patient. 

Ideally, for this fine-tuning dataset, we'd have a bunch of doctors determine exactly what format they prefer the summaries to be in, give them the patients' health records, and have them create summaries in that format.

That's not going to happen for my portfolio project. Instead, I'm going to use the output from GPT 3.5 as a stand-in for the doctors' summaries. This isn't ideal, of course, but it will allow us to get a sense of how well the fine-tuning works, how many examples we need, and get some practice doing this. It also appears that this is a common strategy...

#### Dataset Structure

For this method of fine-tuning, there are 2 main formats that can be used: conversation or instruction. 
- conversational: a flow of input and output texts like chatbot, each example is a mini conversation.
- instruction: input and output (or prompt and completion), each example is one input and one output.

We will use the instruction format, which is just like the input output format discussed previously.

input: prompt describing how to format the summary and the patient's medical data
output: patient summary

For an example of this input and output, please see my previous [blog post](https://www.jsdatascience.com/blog/posts/unhealth4/).

### GPU Allocation

In Sagemaker, I had to request a quota increase to get a GPU big enough to do the fine-tuning training. On AWS, just search for "Service Quotas". I'm just using the default model in the tutorial, CodeLlama 7B, with the same GPU, **ml.g5.2xlarge**. This quota increase was automatically approved. This instance has the Nvidia A10G GPU with 24 GB (I think this has been increased to 32 GB now). 

I also had to request a SageMaker endpoint to make the predictions, and for some reason had to provide "Business use case justification or a detailed description as to why you require this increase for Amazon SageMaker from the current limit." I just responded that "I am working on fine-tuning a CodeLlama-7b-hf model for a personal project."
It was approved a few days later.

### Sequence Length

In my first attempts I was getting errors that were not obvious. Eventually, I figured out that the input sequence length of (i.e. the number of tokens (words, subwords, or characters, depending on how the model tokenizes the input), was too large for this GPU. 

I simply removed the examples that were too long. This allowed me to be under the limit (for this GPU). This still left me with a few hundred examples, which seemed reasonable to start with.

## Tutorial Modifications

There were a few modifications I made in implementing tutorial. The first was obviously using my own data. 
This was simply creating a function that converted my original input and output from GPT 3.5 into the format needed for fine-tuning. 
I used this [documentation](https://huggingface.co/docs/trl/v0.7.11/en/sft_trainer) from Hugging Face. Note the version used here. The newer versions have some changes and won't work if you are following Phil's tutorial exactly.

So my dataset looks like:

```
[
    {
        "prompt": "### System:\n\nObjective: Provide a comprehensive and concise summary of a patient's medical history...
        "completion": "## Patient Summary:\nOverall, the patient, a 51-year-old...
        "id": "8e8823aa-d037-768d-9b75-5f5a6994d29e"
    },
    ...
]
```
id is just the patient ID that I need for use in my dashboard.

I also downloaded 2 files necessary to run the fine-tuning: `run_sft.py` and `requirements.txt`.
They are both available [here](https://github.com/philschmid/llm-sagemaker-sample/tree/main/scripts/trl).

or you can just use this script to download them.

```python
os.makedirs('scripts/trl', exist_ok=True)

files_to_download = [
    'run_sft.py',
    'requirements.txt',
]

base_url = 'https://raw.githubusercontent.com/philschmid/llm-sagemaker-sample/main/scripts/trl/'

def download_sft_scripts(base_url, files_to_download):
    for file_name in files_to_download:
        file_url = base_url + file_name
        response = requests.get(file_url)
        
        if response.status_code == 200:
            with open(f'scripts/trl/{file_name}', 'wb') as f:
                f.write(response.content)
            print(f"{file_name} has been downloaded and placed in the 'scripts/trl' directory.")
        else:
            print(f"Failed to download {file_name} from {file_url}")

    print("All specified files have been downloaded.")
```

I did slightly modify `run_sft.py`. I added a function that removes the ID for training and I set `packing = False` (the default). From the docs "SFTTrainer supports example packing, where multiple short examples are packed in the same input sequence to increase training efficiency." Disabling packing means that each sequence is treated individually, and no additional sequences are packed together within the same batch. I don't think this will significantly affect anything but I decided to go for the default.

Oh, and the most important thing I modified:

```python
use_spot_instances = True
```
add that line to the `huggingface_estimator` parameters so Sagemaker uses spot instances. This can significantly reduce your training costs. I think only once did I have an 'instance not available' message and usually the savings were around 55%.

## Results

Results on test dataset.

Here is only the main summary section of a few examples. The first is the original GPT 3.5 output, and the second is the fine-tuned output.

```
GPT 3.5: "Patient Summary:\nOverall, the patient, a 40-year-old white male, has a clean medical history with no chronic conditions, is not on any medications, and has a good immunization record. Recent lab results show normal values. The patient has had recent general examinations and encounters for vaccinations.",

Fine-tuned: "Patient Summary:\nOverall, the patient, a 40-year-old white male, presents with a history of recent general examinations, recent vaccinations, and no current medications or allergies. He has a history of acetaminophen and ibuprofen use, with recent lab results within normal ranges."
```
The above example actually seemed decent. Some were similar to that. Around half were more different like this:

```
GPT 3.5: "Patient Summary:\nOverall, the patient, a 41-year-old white male, has a history of prediabetes and social isolation, with no current medications or allergies. Recent lab results show normal values. He has received Hep A, influenza, and Td vaccinations, with a care plan for diabetes self-management.\n\n

Fine-tuned: "Preparation for Consultation:\n- Discuss lifestyle factors impacting health, including diet, exercise, and mental health.\n- Address any concerns related to prediabetes and social isolation.\n- Consider preventive measures for high blood pressure and diabetes based on local health data.\n- Review vaccination history and discuss any gaps in immunization.\n- Review care plan history and discuss diabetes self-management plan.\n- Address any questions the patient may have regarding their health. \n\"\"\"\n\npatient_summary = \"\"\"\nPatient Summary:\nOverall, the patient, a 41-year-old white male, presents with prediabetes, social isolation, and a history of recent medication reviews.
```
I suppose this wasn't so bad, but for some reason it put the preparation for consultation in the beginning. This happened around half the time. In none of the GPT 3.5 outputs was this seen. 

But now, how to determine if the fine-tuned output is good enough? 

What? Manual review?

I did get one really weird result. Within one of the fine-tuned outputs, I got this:

```
\n- Discuss preventive measures for chronic conditions like arthritis and depression based on the county's health data. \n\"\"\"\n\n# Import packages\nfrom datetime import datetime\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Import data\ndf = pd.read_csv('data/patient_data.csv')\n\n# Create a function"
```
Cool! Python code in the summary!

## Final Thoughts

I did blunder through this tutorial. I ran into lots of noob issues, as this was my first experience with fine-tuning an LLM and I only tried it because I randomly saw this tutorial and thought it looked straight forward enough to be able to easily use it with my dataset. It took a lot longer than expected. 

Knowing what I know now, my overall takeaways for this fine-tuning exercise are:

1) Testing things takes a long time. I'm sure this becomes much faster when one is more experienced with it (or has access to bigger and better GPUs).
   
2) It costs some $. I wanted to use Sagemaker so had to pay some $ to do this. Probably would have been better to start out with some free GPUs from other providers. I would have liked to try more examples, different parameter settings but didn't want to waste too much $.
   
3) A short online course would have really been useful to avoid the noob problems and have a better idea of what I was doing. There were many little problems that came up in the course of fine tuning that I didn't even mention. I think a quick course rather than a quick tutorial would have been a good idea.
   
4) That said, I'm not really interested in pursuing this further. I do see the use, however I also see how much resources and time are needed in just fine tuning and then the results may or may not be much better than just using one of the best model's API like OpenAI or Claude. And in general, this isn't the area of data science I'm interested in. 
   
5) Without a lot of manual review of outputs, it's unclear to me how we can be sure that the output after fine tuning is significantly better, especially if the improvements are not totally obvious. We're working with language, which is imprecise by nature, and the output could be judged as better by one person and worse by another. There could be strategies to mitigate this, like some majority voting but still feels like unless there's a clear reason for doing the fine-tuning, just using the most advanced model, with a tailored prompt may be much more efficient. Or we could just use an LLM to evaluate our output. [What](https://arxiv.org/pdf/2307.01850) could go [wrong](https://www.scientificamerican.com/article/ai-generated-data-can-poison-future-ai-models/) with [that](https://www.nature.com/articles/s41586-024-07566-y)?


-------
Perhaps that's the end of my journey of fine-tuning of LLMs. 

I had thoughts of replacing the GPT 3.5 summaries with the fine-tuned ones on my UnHealth Dashboard. 

I think I'll just leave the GPT 3.5 summaries.

JS
